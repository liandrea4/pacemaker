{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection \t\t import train_test_split\n",
    "from keras.applications.vgg16 \t\t import VGG16\n",
    "from keras.preprocessing.image       import ImageDataGenerator\n",
    "from keras.models                    import Sequential, Model, Input\n",
    "from keras.layers                    import Dense, Flatten, Dropout\n",
    "from keras.optimizers                import SGD, RMSprop, Adam\n",
    "from keras.callbacks                 import EarlyStopping\n",
    "from keras.utils                     import np_utils\n",
    "from keras                           import backend as K\n",
    "\n",
    "def run(X, Y, pickle_filename, model_filename, batch_size=32, num_epochs=50): \n",
    "\n",
    "\t# Split into test and train/validation set\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, random_state=42, stratify=Y)\n",
    "\n",
    "\t# Split into train and validation\n",
    "\tX_train, X_val, y_train, y_val= train_test_split(X_train, y_train, train_size=0.9, random_state=42, stratify=y_train) \n",
    "\tprint('Train shape: ', X_train.shape, y_train.shape)\n",
    "\tprint('Val shape: ', X_val.shape, y_val.shape)\n",
    "\tprint('Test shape: ', X_test.shape, np.array(y_test).shape)\n",
    "\n",
    "\t# Categorize the labels\n",
    "\tnum_classes = 2\n",
    "\ty_train = np_utils.to_categorical(y_train, num_classes)\n",
    "\ty_val = np_utils.to_categorical(y_val, num_classes)\n",
    "\ty_test = np_utils.to_categorical(y_test, num_classes)\n",
    "\tprint(\"y_train, y_val, y_test: \", y_train.shape, y_val.shape, y_test.shape)\n",
    "\n",
    "\t# Create the base pre-trained model\n",
    "\tbase_model = VGG16(input_shape=(224, 224, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "\tx = base_model.output\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(2048, activation='relu')(x)\n",
    "\tx = Dropout(.7)(x)\n",
    "\tx = Dense(2048, activation='relu')(x)\n",
    "\tpredictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "\tmodel = Model(inputs=base_model.input, outputs=predictions)\n",
    "\t# print(model.summary())\n",
    "\n",
    "\tk = 12 # number of end layers to retrain CHANGE THIS ALISTAIR SAID TO RETRAIN THE LAST CNN5(?) idk how many layers that is (8+4)\n",
    "\tlayers = base_model.layers[:-k] if k != 0 else base_model.layers\n",
    "\tfor layer in layers: \n",
    "\t    layer.trainable = False\n",
    "\n",
    "\t# Compile model\n",
    "\topt = SGD(lr=0.0001, momentum=0.9)\n",
    "\tmodel.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\t# Initiate the train, validation and test generators with data augumentation\n",
    "\ttrain_datagen = ImageDataGenerator(rotation_range=45, zoom_range=.3, rescale = 1./255, horizontal_flip = True, vertical_flip = True)\n",
    "\ttrain_datagen.fit(X_train)\n",
    "\tgenerator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "\tval_datagen = ImageDataGenerator(rotation_range=45, zoom_range=.3, rescale = 1./255, horizontal_flip = True, vertical_flip = True)\n",
    "\tval_datagen.fit(X_val)\n",
    "\tval_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)\n",
    "\n",
    "\ttest_datagen = ImageDataGenerator(rotation_range=45, zoom_range=.3, rescale = 1./255, horizontal_flip = True, vertical_flip = True)\n",
    "\ttest_datagen.fit(X_test)\n",
    "\ttest_generator = test_datagen.flow(X_test, y_test, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\t# Train the model, auto terminating when val_acc stops increasing after 10 epochs.\n",
    "\t# callback = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=2, mode='max') \n",
    "    # , callbacks=[callback],\n",
    "\thist = model.fit_generator(generator, steps_per_epoch=len(X_train) / batch_size , epochs=num_epochs, verbose=1, validation_data=val_generator, validation_steps=len(X_val)/batch_size)\n",
    "\n",
    "\t# Save accuracy / loss during training to pickle file so we can plot later\n",
    "\tpickle.dump(hist.history, open(pickle_filename, 'wb'))\n",
    "\n",
    "\t# Evalulate model\n",
    "\ttest_loss, accuracy = model.evaluate_generator(test_generator, X_test.shape[0])\n",
    "\tprint('Test loss: ', test_loss, ' Accuracy: ', accuracy)\n",
    "\n",
    "\t# Save model\n",
    "\tmodel.save(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_True = pickle.load(open( \"x_true.p\", \"rb\" ))\n",
    "x_False = pickle.load(open( \"x_neg.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_True = [1]*len(x_True)\n",
    "y_False = [0]*len(x_False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5471, 224, 224, 3) (5471,)\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((x_True, x_False))\n",
    "Y = np.concatenate((y_True, y_False))\n",
    "print X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alyssayc/.virtualenvs/venv/local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train shape: ', (3938, 224, 224, 3), (3938,))\n",
      "('Val shape: ', (438, 224, 224, 3), (438,))\n",
      "('Test shape: ', (1095, 224, 224, 3), (1095,))\n",
      "('y_train, y_val, y_test: ', (3938, 2), (438, 2), (1095, 2))\n",
      "Epoch 1/200\n",
      "123/123 [==============================] - 26s 214ms/step - loss: 0.7672 - acc: 0.4983 - val_loss: 0.7649 - val_acc: 0.4495\n",
      "Epoch 2/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.7210 - acc: 0.5297 - val_loss: 0.7013 - val_acc: 0.4495\n",
      "Epoch 3/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.7084 - acc: 0.5107 - val_loss: 0.6959 - val_acc: 0.4495\n",
      "Epoch 4/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6948 - acc: 0.5388 - val_loss: 0.6933 - val_acc: 0.4495\n",
      "Epoch 5/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6959 - acc: 0.5269 - val_loss: 0.6900 - val_acc: 0.5505\n",
      "Epoch 6/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6899 - acc: 0.5427 - val_loss: 0.6939 - val_acc: 0.4495\n",
      "Epoch 7/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6913 - acc: 0.5432 - val_loss: 0.6930 - val_acc: 0.5505\n",
      "Epoch 8/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6902 - acc: 0.5437 - val_loss: 0.6909 - val_acc: 0.5505\n",
      "Epoch 9/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6892 - acc: 0.5445 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 10/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6881 - acc: 0.5453 - val_loss: 0.6887 - val_acc: 0.5505\n",
      "Epoch 11/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6876 - acc: 0.5559 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 12/200\n",
      "123/123 [==============================] - 26s 210ms/step - loss: 0.6900 - acc: 0.5371 - val_loss: 0.6887 - val_acc: 0.5505\n",
      "Epoch 13/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6878 - acc: 0.5541 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 14/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6875 - acc: 0.5539 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 15/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6900 - acc: 0.5462 - val_loss: 0.6884 - val_acc: 0.5505\n",
      "Epoch 16/200\n",
      "123/123 [==============================] - 26s 213ms/step - loss: 0.6891 - acc: 0.5493 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 17/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6869 - acc: 0.5559 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 18/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6892 - acc: 0.5493 - val_loss: 0.6888 - val_acc: 0.5505\n",
      "Epoch 19/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6883 - acc: 0.5506 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 20/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6878 - acc: 0.5531 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 21/200\n",
      "123/123 [==============================] - 26s 209ms/step - loss: 0.6881 - acc: 0.5529 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 22/200\n",
      "123/123 [==============================] - 26s 209ms/step - loss: 0.6876 - acc: 0.5513 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 23/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6869 - acc: 0.5534 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 24/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6894 - acc: 0.5491 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 25/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6889 - acc: 0.5493 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 26/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6898 - acc: 0.5468 - val_loss: 0.6885 - val_acc: 0.5505\n",
      "Epoch 27/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6860 - acc: 0.5622 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 28/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6905 - acc: 0.5432 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 29/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6882 - acc: 0.5518 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 30/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6858 - acc: 0.5625 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 31/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6879 - acc: 0.5533 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 32/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6868 - acc: 0.5597 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 33/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6883 - acc: 0.5528 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 34/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6901 - acc: 0.5425 - val_loss: 0.6885 - val_acc: 0.5505\n",
      "Epoch 35/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6886 - acc: 0.5526 - val_loss: 0.6884 - val_acc: 0.5505\n",
      "Epoch 36/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6880 - acc: 0.5554 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 37/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6867 - acc: 0.5587 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 38/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6883 - acc: 0.5501 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 39/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6861 - acc: 0.5645 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 40/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6903 - acc: 0.5389 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 41/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6871 - acc: 0.5569 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 42/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6855 - acc: 0.5635 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 43/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6898 - acc: 0.5417 - val_loss: 0.6884 - val_acc: 0.5505\n",
      "Epoch 44/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6883 - acc: 0.5523 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 45/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6886 - acc: 0.5463 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 46/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6856 - acc: 0.5665 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 47/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6884 - acc: 0.5480 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 48/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6878 - acc: 0.5546 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 49/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6874 - acc: 0.5562 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 50/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6879 - acc: 0.5531 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 51/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6866 - acc: 0.5582 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 52/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6888 - acc: 0.5506 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 53/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6878 - acc: 0.5562 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 54/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6892 - acc: 0.5449 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 55/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6856 - acc: 0.5628 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 56/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6897 - acc: 0.5414 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 57/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6841 - acc: 0.5691 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 58/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6892 - acc: 0.5468 - val_loss: 0.6884 - val_acc: 0.5505\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6865 - acc: 0.5556 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 60/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6885 - acc: 0.5534 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 61/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6886 - acc: 0.5511 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 62/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6883 - acc: 0.5524 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 63/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6882 - acc: 0.5516 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 64/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6862 - acc: 0.5615 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 65/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6905 - acc: 0.5409 - val_loss: 0.6887 - val_acc: 0.5505\n",
      "Epoch 66/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6851 - acc: 0.5679 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 67/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6901 - acc: 0.5432 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 68/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6862 - acc: 0.5589 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 69/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6857 - acc: 0.5617 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 70/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6875 - acc: 0.5536 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 71/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6876 - acc: 0.5547 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 72/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6895 - acc: 0.5455 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 73/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6873 - acc: 0.5564 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 74/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6872 - acc: 0.5533 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 75/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6891 - acc: 0.5468 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 76/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6873 - acc: 0.5551 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 77/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6871 - acc: 0.5567 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 78/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6884 - acc: 0.5503 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 79/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6868 - acc: 0.5559 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 80/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6900 - acc: 0.5445 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 81/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6894 - acc: 0.5440 - val_loss: 0.6884 - val_acc: 0.5505\n",
      "Epoch 82/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6873 - acc: 0.5566 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 83/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6883 - acc: 0.5523 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 84/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6863 - acc: 0.5602 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 85/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6863 - acc: 0.5607 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 86/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6871 - acc: 0.5549 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 87/200\n",
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6871 - acc: 0.5549 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 88/200\n",
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6865 - acc: 0.5599 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 89/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6884 - acc: 0.5503 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 90/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6885 - acc: 0.5488 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 91/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6879 - acc: 0.5526 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 92/200\n",
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6868 - acc: 0.5567 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 93/200\n",
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6877 - acc: 0.5531 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 94/200\n",
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6869 - acc: 0.5569 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 95/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6884 - acc: 0.5488 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 96/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6886 - acc: 0.5486 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 97/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6885 - acc: 0.5503 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 98/200\n",
      "123/123 [==============================] - 24s 199ms/step - loss: 0.6887 - acc: 0.5445 - val_loss: 0.6884 - val_acc: 0.5505\n",
      "Epoch 99/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6862 - acc: 0.5595 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 100/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6893 - acc: 0.5473 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 101/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6865 - acc: 0.5587 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 102/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6883 - acc: 0.5511 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 103/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6867 - acc: 0.5592 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 104/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6867 - acc: 0.5582 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 105/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6879 - acc: 0.5523 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 106/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6873 - acc: 0.5539 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 107/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6884 - acc: 0.5498 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 108/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6863 - acc: 0.5605 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 109/200\n",
      "123/123 [==============================] - 24s 196ms/step - loss: 0.6871 - acc: 0.5554 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 110/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6885 - acc: 0.5508 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 111/200\n",
      "123/123 [==============================] - 24s 195ms/step - loss: 0.6882 - acc: 0.5501 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 112/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6885 - acc: 0.5506 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 113/200\n",
      "123/123 [==============================] - 24s 195ms/step - loss: 0.6875 - acc: 0.5534 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 114/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6868 - acc: 0.5579 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 115/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6866 - acc: 0.5554 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 116/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6880 - acc: 0.5516 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 117/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6881 - acc: 0.5528 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 118/200\n",
      "123/123 [==============================] - 24s 196ms/step - loss: 0.6873 - acc: 0.5549 - val_loss: 0.6881 - val_acc: 0.5505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6882 - acc: 0.5521 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 120/200\n",
      "123/123 [==============================] - 24s 196ms/step - loss: 0.6885 - acc: 0.5514 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 121/200\n",
      "123/123 [==============================] - 24s 195ms/step - loss: 0.6887 - acc: 0.5481 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 122/200\n",
      "123/123 [==============================] - 24s 195ms/step - loss: 0.6892 - acc: 0.5468 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 123/200\n",
      "123/123 [==============================] - 24s 194ms/step - loss: 0.6881 - acc: 0.5514 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 124/200\n",
      "123/123 [==============================] - 24s 193ms/step - loss: 0.6881 - acc: 0.5531 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 125/200\n",
      "123/123 [==============================] - 24s 194ms/step - loss: 0.6878 - acc: 0.5528 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 126/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6883 - acc: 0.5508 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 127/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6880 - acc: 0.5539 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 128/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6888 - acc: 0.5486 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 129/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6879 - acc: 0.5516 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 130/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6894 - acc: 0.5453 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 131/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6870 - acc: 0.5541 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 132/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6879 - acc: 0.5521 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 133/200\n",
      "123/123 [==============================] - 26s 210ms/step - loss: 0.6881 - acc: 0.5503 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 134/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6870 - acc: 0.5572 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 135/200\n",
      "123/123 [==============================] - 26s 209ms/step - loss: 0.6896 - acc: 0.5452 - val_loss: 0.6883 - val_acc: 0.5505\n",
      "Epoch 136/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6869 - acc: 0.5587 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 137/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6862 - acc: 0.5604 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 138/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6878 - acc: 0.5539 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 139/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6890 - acc: 0.5482 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "Epoch 140/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6864 - acc: 0.5609 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 141/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6886 - acc: 0.5485 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 142/200\n",
      "123/123 [==============================] - 26s 209ms/step - loss: 0.6864 - acc: 0.5574 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 143/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6878 - acc: 0.5531 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 144/200\n",
      "123/123 [==============================] - 26s 212ms/step - loss: 0.6874 - acc: 0.5554 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 145/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6890 - acc: 0.5480 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 146/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6861 - acc: 0.5602 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 147/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6877 - acc: 0.5541 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 148/200\n",
      "123/123 [==============================] - 26s 210ms/step - loss: 0.6879 - acc: 0.5516 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 149/200\n",
      "123/123 [==============================] - 26s 208ms/step - loss: 0.6873 - acc: 0.5566 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 150/200\n",
      "123/123 [==============================] - 26s 209ms/step - loss: 0.6867 - acc: 0.5559 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 151/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6885 - acc: 0.5491 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 152/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6875 - acc: 0.5551 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 153/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6887 - acc: 0.5473 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 154/200\n",
      "123/123 [==============================] - 25s 207ms/step - loss: 0.6854 - acc: 0.5635 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 155/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6873 - acc: 0.5541 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 156/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6871 - acc: 0.5564 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 157/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6887 - acc: 0.5470 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 158/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6878 - acc: 0.5528 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 159/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6858 - acc: 0.5617 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 160/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6882 - acc: 0.5503 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 161/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6861 - acc: 0.5605 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 162/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6886 - acc: 0.5480 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 163/200\n",
      "123/123 [==============================] - 25s 206ms/step - loss: 0.6857 - acc: 0.5617 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 164/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6880 - acc: 0.5513 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 165/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6886 - acc: 0.5475 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 166/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6885 - acc: 0.5496 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 167/200\n",
      "123/123 [==============================] - 25s 205ms/step - loss: 0.6887 - acc: 0.5483 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 168/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6861 - acc: 0.5600 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 169/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6879 - acc: 0.5516 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 170/200\n",
      "123/123 [==============================] - 26s 207ms/step - loss: 0.6880 - acc: 0.5505 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 171/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6866 - acc: 0.5582 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 172/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6880 - acc: 0.5513 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 173/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6875 - acc: 0.5541 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 174/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6878 - acc: 0.5536 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 175/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6870 - acc: 0.5551 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 176/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6890 - acc: 0.5468 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 177/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6855 - acc: 0.5628 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 178/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6885 - acc: 0.5483 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 179/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6882 - acc: 0.5493 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 180/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6871 - acc: 0.5567 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 181/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6841 - acc: 0.5668 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 182/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6905 - acc: 0.5422 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 183/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6857 - acc: 0.5610 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 184/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6906 - acc: 0.5391 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 185/200\n",
      "123/123 [==============================] - 25s 204ms/step - loss: 0.6860 - acc: 0.5607 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 186/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6894 - acc: 0.5445 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 187/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6870 - acc: 0.5564 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 188/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6871 - acc: 0.5554 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 189/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6866 - acc: 0.5582 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 190/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6881 - acc: 0.5501 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 191/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6881 - acc: 0.5511 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 192/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6853 - acc: 0.5635 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 193/200\n",
      "123/123 [==============================] - 24s 197ms/step - loss: 0.6895 - acc: 0.5445 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 194/200\n",
      "123/123 [==============================] - 25s 202ms/step - loss: 0.6882 - acc: 0.5511 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 195/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6876 - acc: 0.5537 - val_loss: 0.6881 - val_acc: 0.5505\n",
      "Epoch 196/200\n",
      "123/123 [==============================] - 25s 203ms/step - loss: 0.6877 - acc: 0.5531 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 197/200\n",
      "123/123 [==============================] - 25s 201ms/step - loss: 0.6882 - acc: 0.5518 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 198/200\n",
      "123/123 [==============================] - 24s 198ms/step - loss: 0.6865 - acc: 0.5579 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 199/200\n",
      "123/123 [==============================] - 25s 200ms/step - loss: 0.6883 - acc: 0.5516 - val_loss: 0.6880 - val_acc: 0.5505\n",
      "Epoch 200/200\n",
      "123/123 [==============================] - 25s 199ms/step - loss: 0.6904 - acc: 0.5392 - val_loss: 0.6882 - val_acc: 0.5505\n",
      "('Test loss: ', 0.68767459939800524, ' Accuracy: ', 0.55359696492380983)\n"
     ]
    }
   ],
   "source": [
    "run(X, Y, 'full_hist.pkl', 'full_model.h5', batch_size=32, num_epochs=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
